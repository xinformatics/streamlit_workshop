{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiKJBggnzos++mHWKKnkRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinformatics/streamlit_workshop/blob/main/02_Workshop_Feb_05_2024_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4zFCFeC6Qx3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9ec2d9-1b73-4fe8-e19a-9a1c5f1fd4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.30.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.0.1)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.17.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.41 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.30.0 validators-0.22.0 watchdog-3.0.0\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.74s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h--2024-01-31 23:35:52--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2024.1.5/cloudflared-linux-amd64 [following]\n",
            "--2024-01-31 23:35:53--  https://github.com/cloudflare/cloudflared/releases/download/2024.1.5/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/3e1a9578-ca20-4ec0-aead-3080c5647407?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240131%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240131T233553Z&X-Amz-Expires=300&X-Amz-Signature=ec0d2a964fa7dd8bb6d0a8fba463ddbccfe41de8ac2e1986c72f185b6379d942&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-01-31 23:35:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/3e1a9578-ca20-4ec0-aead-3080c5647407?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240131%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240131T233553Z&X-Amz-Expires=300&X-Amz-Signature=ec0d2a964fa7dd8bb6d0a8fba463ddbccfe41de8ac2e1986c72f185b6379d942&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36501267 (35M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  34.81M   162MB/s    in 0.2s    \n",
            "\n",
            "2024-01-31 23:35:53 (162 MB/s) - ‘cloudflared-linux-amd64’ saved [36501267/36501267]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "!npm install localtunnel\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x cloudflared-linux-amd64\n",
        "!nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8y6qDQpdxM1",
        "outputId": "b1cac369-348f-470c-a77e-0d271436743d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1 | xargs -I {} echo \"Your tunnel url {}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBRD8JPgdxf-",
        "outputId": "646f8273-f1b5-4b0e-df9f-e654ec5e6340"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tunnel url https://talent-payroll-sh-estimation.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "# Title of the application\n",
        "st.title('My First ML model deployment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rph3P7Mdxju",
        "outputId": "a6145705-c285-4143-ffe1-9369728b41f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "\n",
        "## once it's running you can keep adding text without shuting the app down."
      ],
      "metadata": {
        "id": "ZCK3DEN7dxlJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Computer Vision Example- Building an Image Classifier with InceptionV3**\n",
        "\n"
      ],
      "metadata": {
        "id": "u0QOR8Lp-Ek1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "\n",
        "# Initialize the InceptionV3 model\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "\n",
        "def classify_image(img):\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    preds = model.predict(img)\n",
        "    return decode_predictions(preds, top=10)[0]\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Classification with InceptionV3\")\n",
        "    st.write(\"Upload an image and the model will classify it into the top 10 categories.\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "    if uploaded_file is not None:\n",
        "        img = image.load_img(uploaded_file, target_size=(299, 299))\n",
        "        st.image(img, caption='Uploaded Image', use_column_width=False)\n",
        "        st.write(\"\")\n",
        "        st.write(\"Classifying...\")\n",
        "        labels = classify_image(img)\n",
        "        for label in labels:\n",
        "            st.write(f\"{label[1]} ({label[2]*100:.2f}%)\")\n",
        "            st.progress(int(label[2] * 100))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US8wGqJ3dxon",
        "outputId": "282a4574-3b11-429b-a367-ca0758cb2bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Computer Vision Example - Object Detection**"
      ],
      "metadata": {
        "id": "IV0f2TD5rRDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "import numpy as np\n",
        "import colorsys\n",
        "\n",
        "# Define COCO labels mapping\n",
        "COCO_LABELS = {\n",
        "    1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',    6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',\n",
        "    11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench',    16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep',\n",
        "    21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe',    27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie',\n",
        "    33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard',    37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove',\n",
        "    41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle',    46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon',\n",
        "    51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange',    56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut',\n",
        "    61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed',    67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse',\n",
        "    75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave',    79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book',\n",
        "    85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier',    90: 'toothbrush'}\n",
        "\n",
        "\n",
        "def get_color(label_id):\n",
        "    # Generate a unique color for each label id based on its hash value\n",
        "    hue = hash(label_id) % 360  # Limit hue to [0, 360]\n",
        "    saturation = 90 + hash(label_id) % 10  # Saturation between [90, 100]\n",
        "    lightness = 50 + hash(label_id) % 10  # Lightness between [50, 60]\n",
        "\n",
        "    color = colorsys.hls_to_rgb(hue / 360, lightness / 100, saturation / 100)\n",
        "    # Convert to RGB format as integer tuple\n",
        "    return tuple(int(c * 255) for c in color)\n",
        "\n",
        "@st.cache_resource()\n",
        "def load_model():\n",
        "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "def object_detection(image, model):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image)\n",
        "\n",
        "    return prediction[0]\n",
        "\n",
        "def main():\n",
        "    st.title(\"Object Detection with FasterRCNN\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "    if uploaded_file is not None:\n",
        "        image = Image.open(uploaded_file).convert('RGB')\n",
        "        st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "        prediction = object_detection(image, model)\n",
        "\n",
        "        # Prepare to draw\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        # Define font for drawing text\n",
        "        font_size = 20\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "\n",
        "        for element in range(len(prediction[\"boxes\"])):\n",
        "            boxes = prediction[\"boxes\"][element].cpu().numpy()\n",
        "            score = np.round(prediction[\"scores\"][element].cpu().numpy(), decimals=4)\n",
        "            label_id = int(prediction[\"labels\"][element].cpu().numpy())\n",
        "            label_name = COCO_LABELS.get(label_id, 'N/A')\n",
        "            color = get_color(label_id)\n",
        "            text_color = (255, 255, 255)  # White color for text\n",
        "\n",
        "            if score > 0.5:\n",
        "                label_with_score = f\"{label_name}: {score:.2f}\"\n",
        "                draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], outline=color, width=3)\n",
        "                draw.text((boxes[0], boxes[1] - 10), label_with_score, fill=text_color, font=font)\n",
        "\n",
        "        st.image(image, caption=\"Processed Image\", use_column_width=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00749QqSdxwH",
        "outputId": "32678bdf-3348-413a-c8af-3c83b1dbe787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUGaPaP63jF-",
        "outputId": "8bf95493-f1e5-4c92-db98-03e9ca702480"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing - Emotion Detection**"
      ],
      "metadata": {
        "id": "2cthfe6X-c7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please download the model weights and rename it to model.hdf5**"
      ],
      "metadata": {
        "id": "tQlEVoKT7Hr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget -L https://www.dropbox.com/scl/fi/svc8wvbl6c5dxwcu3xjw3/model3.h5?rlkey=pe2yn2w1ffrszpc6ex9rgf50m&dl=0\n",
        "\n"
      ],
      "metadata": {
        "id": "xmbpp-penuU0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "### Given a text, predict whether any of the following emotions are present:\n",
        "\n",
        "\n",
        "*   admiration\n",
        "*   amusement\n",
        "*   gratitude\n",
        "*   love\n",
        "*   pride\n",
        "*   relief\n",
        "*   remorse\n",
        "\n",
        "\n",
        "For example, given the text:\n",
        "\n",
        "Thanks for the reply! I appreciate your input. Please keep me in the loop, I’d love to be more active with this if possible.\n",
        "\n",
        "\n",
        "Output\"\n",
        "\n",
        "admiration, gratitude, love"
      ],
      "metadata": {
        "id": "A9sONqOs_7QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "def main():\n",
        "    import streamlit as st\n",
        "    # from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "    from transformers import BertTokenizer, BertModel, TFBertModel\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "\n",
        "    emotions = [\"admiration\",\"amusement\",\"gratitude\",\"love\",\"pride\",\"relief\",\"remorse\"]\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    # model_path = '/content/model3.h5'\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    # model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
        "    model = tf.keras.models.load_model('/content/model.hdf5', custom_objects={\"TFBertModel\": TFBertModel})\n",
        "\n",
        "    def predict_emotion(text):\n",
        "        test_input_ids=[]\n",
        "        test_attention_masks=[]\n",
        "        # for sent in texts_train:\n",
        "        bert_inp_train=tokenizer.encode_plus(text,add_special_tokens = True,max_length =35,pad_to_max_length = True,return_attention_mask = True, truncation=True)\n",
        "        test_input_ids.append(bert_inp_train['input_ids'])\n",
        "        test_attention_masks.append(bert_inp_train['attention_mask'])\n",
        "\n",
        "        test_input_ids=np.asarray(test_input_ids)\n",
        "        test_attention_masks=np.array(test_attention_masks)\n",
        "\n",
        "        # inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
        "        # outputs = model(inputs)\n",
        "        # predictions = np.where(model.predict([test_input_ids,test_attention_masks]) > 0.5, 1, 0)\n",
        "\n",
        "        predictions = model.predict([test_input_ids,test_attention_masks])[0]\n",
        "        # logits = outputs.logits\n",
        "        # predicted_class_id = tf.argmax(logits, axis=1).numpy()[0]\n",
        "        # return model.config.id2label[predicted_class_id]\n",
        "        return predictions\n",
        "\n",
        "    # Streamlit interface\n",
        "    st.title('Emotion Detection in Text using the BERT Model')\n",
        "    user_input = st.text_area(\"Enter the text you want to analyze\", \"\")\n",
        "    if st.button('Analyze'):\n",
        "        if user_input:\n",
        "            probabilities = predict_emotion(user_input)\n",
        "            scaled_probabilities = [int(probability * 100) for probability in probabilities]  # Scale and convert to int\n",
        "            # st.write(f'Predicted Emotion: {prediction}')\n",
        "            for emotion, prob_int in zip(emotions, scaled_probabilities):\n",
        "                st.write(f\"{emotion}:\")\n",
        "                st.progress(prob_int)\n",
        "        else:\n",
        "            st.write('Please enter some text to analyze.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "PKrjv8rxdxyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b00631-0220-452f-fdd3-ee06a1203e49"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2S8O7zGdx1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsOpRv-Bdx4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AucKmkrdx6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sf-h-S_bdx9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6Js4LVddyAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyCEWw2ldyDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MnT2SEXTdyF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbbV-TpfdyIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IsL7ksyKdyLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PU1MiajYdyOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}